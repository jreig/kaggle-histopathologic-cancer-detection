{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Set up environment for GPU (!important)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print(\"TF version\", tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# If this last line give an error, stop the notebook kernel, reset it and run again"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Train module\n",
    "\n",
    "## Import required modules to process data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Train configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE      = 96    # Images are 96x96 px\n",
    "IMAGE_CHANNELS  = 3     # Images are 3 chanell (RGB)"
   ]
  },
  {
   "source": [
    "## Load train data info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append image file extension to train img ids\n",
    "def appendExt(id):\n",
    "    return id + \".tif\"\n",
    "\n",
    "# Load CSVs\n",
    "traindf = pd.read_csv(\"/dataset/train_labels.csv\")\n",
    "\n",
    "# Add extensions to id files\n",
    "traindf[\"id\"] = traindf[\"id\"].apply(appendExt)\n",
    "\n",
    "# Labels must be strings\n",
    "traindf[\"label\"] = traindf[\"label\"].astype(str)\n",
    "\n",
    "# removing this image because it caused a training error previously\n",
    "traindf[traindf['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "traindf[traindf['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']"
   ]
  },
  {
   "source": [
    "## Build image data generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe = traindf,\n",
    "    directory = \"/dataset/train/\",\n",
    "    x_col = \"id\",\n",
    "    y_col = \"label\",\n",
    "    subset = \"training\",\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 10,\n",
    "    shuffle = True,\n",
    "    class_mode = \"binary\",\n",
    ")\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "    dataframe = traindf,\n",
    "    directory = \"/dataset/train/\",\n",
    "    x_col = \"id\",\n",
    "    y_col = \"label\",\n",
    "    subset = \"validation\",\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 10,\n",
    "    shuffle = True,\n",
    "    class_mode = \"binary\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate class weigths\n",
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(traindf['label']),\n",
    "    y=traindf['label']\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "source": [
    "## Build example model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizers.Adam(lr=0.0001), \n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Train the example model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "print(\"STEP_SIZE_TRAIN:\", STEP_SIZE_TRAIN)\n",
    "print(\"STEP_SIZE_VALID:\", STEP_SIZE_VALID)\n",
    "\n",
    "# Save best model\n",
    "checkpointPath = \"/usr/src/scripts/best-model.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpointPath,\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Dynamic learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=2, \n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    min_lr=0.00001\n",
    ")\n",
    "                                                                \n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=1, # Only for test!\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(valid_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "source": [
    "## Predict test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load test data\n",
    "# testdf = pd.read_csv(\"/dataset/sample_submission.csv\")\n",
    "# testdf[\"id\"] = testdf[\"id\"].apply(appendExt)\n",
    "\n",
    "# # Set up test data generator (only apply normalization)\n",
    "# test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "# test_generator=test_datagen.flow_from_dataframe(\n",
    "#     dataframe=testdf,\n",
    "#     directory=\"/dataset/test/\",\n",
    "#     x_col=\"id\",\n",
    "#     y_col=None,\n",
    "#     batch_size=10,\n",
    "#     shuffle=False,\n",
    "#     class_mode=None,\n",
    "#     target_size=(IMAGE_SIZE,IMAGE_SIZE)\n",
    "# )\n",
    "\n",
    "\n",
    "# STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_generator.reset()\n",
    "# model.predict(test_generator, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}