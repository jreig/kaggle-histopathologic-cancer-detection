{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Set up environment for GPU (!important)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print(\"TF version\", tf.__version__)\n",
    "print(\"keras version:\", keras.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "   tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# If this last line give an error, stop the notebook kernel, reset it and run again"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TF version 2.4.1\nkeras version: 2.4.3\nNum GPUs Available:  1\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Train module\n",
    "\n",
    "## Import required modules to process data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## Train configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE      = 96    # Images are 96x96 px\n",
    "IMAGE_CHANNELS  = 3     # Images are 3 chanell (RGB)"
   ]
  },
  {
   "source": [
    "## Load train data info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                  id label\n",
       "0       f38a6374c348f90b587e046aac6079959adf3835.tif     0\n",
       "1       c18f2d887b7ae4f6742ee445113fa1aef383ed77.tif     1\n",
       "2       755db6279dae599ebb4d39a9123cce439965282d.tif     0\n",
       "3       bc3f0c64fb968ff4a8bd33af6971ecae77c75e08.tif     0\n",
       "4       068aba587a4950175d04c680d38943fd488d6a9d.tif     0\n",
       "...                                              ...   ...\n",
       "220020  53e9aa9d46e720bf3c6a7528d1fca3ba6e2e49f6.tif     0\n",
       "220021  d4b854fe38b07fe2831ad73892b3cec877689576.tif     1\n",
       "220022  3d046cead1a2a5cbe00b2b4847cfb7ba7cf5fe75.tif     0\n",
       "220023  f129691c13433f66e1e0671ff1fe80944816f5a2.tif     0\n",
       "220024  a81f84895ddcd522302ddf34be02eb1b3e5af1cb.tif     1\n",
       "\n",
       "[220025 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f38a6374c348f90b587e046aac6079959adf3835.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77.tif</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>755db6279dae599ebb4d39a9123cce439965282d.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>068aba587a4950175d04c680d38943fd488d6a9d.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>220020</th>\n      <td>53e9aa9d46e720bf3c6a7528d1fca3ba6e2e49f6.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>220021</th>\n      <td>d4b854fe38b07fe2831ad73892b3cec877689576.tif</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>220022</th>\n      <td>3d046cead1a2a5cbe00b2b4847cfb7ba7cf5fe75.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>220023</th>\n      <td>f129691c13433f66e1e0671ff1fe80944816f5a2.tif</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>220024</th>\n      <td>a81f84895ddcd522302ddf34be02eb1b3e5af1cb.tif</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>220025 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Function to append image file extension to train img ids\n",
    "def appendExt(id):\n",
    "    return id + \".tif\"\n",
    "\n",
    "# Load CSVs\n",
    "traindf = pd.read_csv(\"/dataset/train_labels.csv\")\n",
    "\n",
    "# ========= FOR PROTOTYPING ONLY =========== #\n",
    "# traindf = traindf[:100]\n",
    "# ========================================== #\n",
    "\n",
    "# Add extensions to id files\n",
    "traindf[\"id\"] = traindf[\"id\"].apply(appendExt)\n",
    "\n",
    "# Labels must be strings\n",
    "traindf[\"label\"] = traindf[\"label\"].astype(str)\n",
    "\n",
    "# removing this image because it caused a training error previously\n",
    "traindf[traindf['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n",
    "\n",
    "# removing this image because it's black\n",
    "traindf[traindf['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']"
   ]
  },
  {
   "source": [
    "## Build image data generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 165019 validated image filenames belonging to 2 classes.\n",
      "Found 55006 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.25)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe = traindf,\n",
    "    directory = \"/dataset/train/\",\n",
    "    x_col = \"id\",\n",
    "    y_col = \"label\",\n",
    "    subset = \"training\",\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 10,\n",
    "    shuffle = True,\n",
    "    class_mode = \"categorical\",\n",
    ")\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "    dataframe = traindf,\n",
    "    directory = \"/dataset/train/\",\n",
    "    x_col = \"id\",\n",
    "    y_col = \"label\",\n",
    "    subset = \"validation\",\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 10,\n",
    "    shuffle = True,\n",
    "    class_mode = \"categorical\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class weights: {0: 0.840380267057781, 1: 1.234472659537462}\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weigths\n",
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(traindf['label']),\n",
    "    y=traindf['label']\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "source": [
    "## Build example model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 94, 94, 32)        896       \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 92, 92, 32)        9248      \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 90, 90, 32)        9248      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 45, 45, 32)        0         \n_________________________________________________________________\ndropout (Dropout)            (None, 45, 45, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 43, 43, 64)        18496     \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 41, 41, 64)        36928     \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 39, 39, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 19, 19, 64)        0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 17, 17, 128)       73856     \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 13, 13, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 6, 6, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4608)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               1179904   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 514       \n=================================================================\nTotal params: 1,661,186\nTrainable params: 1,661,186\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "kernel_size = (3,3)\n",
    "pool_size= (2,2)\n",
    "first_filters = 32\n",
    "second_filters = 64\n",
    "third_filters = 128\n",
    "\n",
    "dropout_conv = 0.3\n",
    "dropout_dense = 0.3\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size)) \n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(second_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(Conv2D(third_filters, kernel_size, activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size = pool_size))\n",
    "model.add(Dropout(dropout_conv))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(dropout_dense))\n",
    "model.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizers.Adam(lr=0.0001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Train the example model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STEP_SIZE_TRAIN: 16501\nSTEP_SIZE_VALID: 5500\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "print(\"STEP_SIZE_TRAIN:\", STEP_SIZE_TRAIN)\n",
    "print(\"STEP_SIZE_VALID:\", STEP_SIZE_VALID)\n",
    "\n",
    "# Save best model\n",
    "checkpointPath = \"/usr/src/scripts/best-model.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpointPath,\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Dynamic learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=2, \n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    min_lr=0.00001\n",
    ")\n",
    "                                                                \n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "16501/16501 [==============================] - 279s 17ms/step - loss: 0.4872 - accuracy: 0.7675 - val_loss: 0.5604 - val_accuracy: 0.7587\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75871, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 2/10\n",
      "16501/16501 [==============================] - 269s 16ms/step - loss: 0.3555 - accuracy: 0.8458 - val_loss: 0.3169 - val_accuracy: 0.8666\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.75871 to 0.86656, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 3/10\n",
      "16501/16501 [==============================] - 257s 16ms/step - loss: 0.3053 - accuracy: 0.8721 - val_loss: 0.2665 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.86656 to 0.88782, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 4/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.2698 - accuracy: 0.8913 - val_loss: 0.2903 - val_accuracy: 0.8755\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.88782\n",
      "Epoch 5/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.2514 - accuracy: 0.8993 - val_loss: 0.2367 - val_accuracy: 0.9053\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.88782 to 0.90535, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 6/10\n",
      "16501/16501 [==============================] - 252s 15ms/step - loss: 0.2344 - accuracy: 0.9077 - val_loss: 0.2248 - val_accuracy: 0.9140\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.90535 to 0.91398, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 7/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.2195 - accuracy: 0.9145 - val_loss: 0.1970 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.91398 to 0.92585, saving model to /usr/src/scripts/best-model.h5\n",
      "Epoch 8/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.2096 - accuracy: 0.9191 - val_loss: 0.2133 - val_accuracy: 0.9174\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.92585\n",
      "Epoch 9/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.2055 - accuracy: 0.9209 - val_loss: 0.2120 - val_accuracy: 0.9185\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.92585\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Epoch 10/10\n",
      "16501/16501 [==============================] - 253s 15ms/step - loss: 0.1774 - accuracy: 0.9332 - val_loss: 0.1933 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.92585\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa31c058eb8>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,\n",
    "    epochs=10, # Only for test!\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5500/5500 [==============================] - 36s 7ms/step - loss: 0.1932 - accuracy: 0.9244\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_accuracy = model.evaluate(valid_generator, steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "source": [
    "## Predict test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 57458 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "testdf = pd.read_csv(\"/dataset/sample_submission.csv\")\n",
    "testdf[\"id\"] = testdf[\"id\"].apply(appendExt)\n",
    "\n",
    "# Set up test data generator (only apply normalization)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "    dataframe=testdf,\n",
    "    directory=\"/dataset/test/\",\n",
    "    x_col=\"id\",\n",
    "    y_col=None,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    target_size=(IMAGE_SIZE,IMAGE_SIZE)\n",
    ")\n",
    "\n",
    "\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.9757699 , 0.02423015],\n",
       "       [0.61150044, 0.38849956],\n",
       "       [0.98754615, 0.01245388],\n",
       "       ...,\n",
       "       [0.11107685, 0.88892317],\n",
       "       [0.72510606, 0.27489388],\n",
       "       [0.99171174, 0.00828821]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "model.predict(test_generator, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}